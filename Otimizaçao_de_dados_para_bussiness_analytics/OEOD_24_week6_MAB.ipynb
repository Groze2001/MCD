{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    " <div class=\"alert alert-block alert-info\"><b>Master in Data Science - Iscte <b>\n",
    "     </div>\n",
    "</font> \n",
    " \n",
    " \n",
    "     \n",
    "    \n",
    "  <font size=\"5\"> OEOD </font>\n",
    "  \n",
    "  \n",
    "  \n",
    "  <font size=\"3\"> **Diana Aldea Mendes**, November 2024 </font>\n",
    "  \n",
    "   \n",
    "  <font size=\"3\"> *diana.mendes@iscte-iul.pt* </font> \n",
    "  \n",
    "    \n",
    " \n",
    "  \n",
    "    \n",
    "  <font color='blue'><font size=\"5\"> <b>Week 6 - Multi-Armed Bandits (MAB)<b></font></font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core concepts\n",
    "\n",
    "## One armed bandits \n",
    "\n",
    "- The one armed bandit is a slang for the *slot machines* in casinos.\n",
    "- Action: pulling of the lewer. \n",
    "- Each action costs a certain amount of money. \n",
    "- The reward is unknown. \n",
    "- For a fixed budget of money and multiple slot machines, we need a winning strategy - that is, what machine to pull?\n",
    "- If we spend all money on one slot machine, we will never know what is the average return of another. \n",
    "\n",
    "## K - armed bandits \n",
    "\n",
    "- $K$ refers to all the possible actions that an agent can take at a given time step $t$ (choose to pull one of the $K$ levers). \n",
    "- The simplest formulation of the k-armed problem is `to find which bandit gives the highest reward without any other piece of information`. \n",
    "\n",
    "- A more standart formulation is to `maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps`.\n",
    "\n",
    "## K - armed bandit simulation \n",
    "\n",
    "- Let us assume that we know the distribution of returns for each bandit. \n",
    "- We can simulate the returns of each bandit by sampling from the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:10:05.162360Z",
     "start_time": "2024-11-18T21:10:03.991229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the random number generator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing libraries for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Dataframe creation \n",
    "import pandas as pd\n",
    "\n",
    "# Iteration tracking \n",
    "from tqdm import tqdm \n",
    "\n",
    "# high-quality figures\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:10:22.494637Z",
     "start_time": "2024-11-18T21:10:22.407538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating 5 bandits each with different normal distributions\n",
    "# define list with different means\n",
    "means = [5, 6.5, 7, 8.5, 9.5]\n",
    "\n",
    "# Generating the returns for each bandit\n",
    "bandits = [np.random.normal(m, 1, 1000) for m in means]\n",
    "\n",
    "# Creating a dataframe for the returns\n",
    "df = pd.DataFrame(bandits).T \n",
    "\n",
    "# Melting the dataframe\n",
    "df = pd.melt(df)\n",
    "df.columns=['bandit', 'return']\n",
    "\n",
    "# Ploting the returns for each bandit\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.violinplot(x='bandit', y='return', data=df, ax=ax)\n",
    "\n",
    "# Adding the labels of  the mean returns\n",
    "for i, m in enumerate(means):\n",
    "    # The background color of the text is set to white\n",
    "    ax.text(i, m, f'{m:.2f}', ha='center', va='center', color='white', fontsize=15, fontweight='bold')\n",
    "\n",
    "ax.set_title('Bandit returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that: since we create the bandits, we know that the best strategy is to always play on the 5th bandit\n",
    "### because the mean reward after each pull is the highest. \n",
    "### The problem is that an agent does not know this. It has to learn by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation vs exploration \n",
    "\n",
    "- Fundamental problems in RL: explotation vs exploration.  \n",
    "\n",
    "- **Exploration**: allows an agent to improve current knowledge about each action, hopefully leading to long-term benefit. Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future.\n",
    "\n",
    "- **Exploitation**: chooses the greedy action to get the most reward by exploiting the agentâ€™s current action-value estimates. Being greedy with respect to action-value estimates, can lead the agent to a sub-optimal behaviour (not get the best reward). \n",
    "\n",
    "- When an agent explores, it gets more accurate estimates of **action-values**. \n",
    "- When an agent exploits, it might get more reward. \n",
    "- Problem: The agent cannot choose to do both simultaneously (`exploration-exploitation dilemma`).\n",
    "\n",
    "## Notations \n",
    "\n",
    "- $\\mathbb{A} = \\{a_{1}, a_{2}, ..., a_{n}\\}$ - set of available actions the agent can take.  \n",
    "\n",
    "- $A_{t}$ - action taken at time step $t$.\n",
    "\n",
    "- $R_{t} \\in \\mathbb{R}$ - the reward at time step $t$. \n",
    "\n",
    "- $q_{*}(a) = \\mathbb{E}\\left[R_{t} | A_{t} = a\\right]$ - the expected value of action $a$, also called the optimal action value. \n",
    "\n",
    "- $Q_{t}(a)$ - the estimated value of action $a$ at time step $t$. \n",
    "- **Goal**: estimate this value as close as possible to the optimal action value $q_{*}(a)$. \n",
    "\n",
    "## Exploration and exploitation in k - armed bandits \n",
    "\n",
    "### Action - value estimation \n",
    "\n",
    "- For the 5 armed bandit example, the set of all possible actions is: \n",
    "\n",
    "$$ A = \\{0, 1, 2, 3, 4\\} $$\n",
    "\n",
    "- The action value estimate at time step $t$ for a given action $a$ has a recursive definition:\n",
    "\n",
    "$$ Q_{t}(a) = Q_{t-1}(a) + \\alpha \\left[R_{t-1} - Q_{t-1}(a)\\right] $$\n",
    "\n",
    "$ \\forall a \\in A$, $\\alpha \\in [0, 1]$\n",
    "\n",
    "- The greedy action is an action that: \n",
    "\n",
    "$$ a_{greedy} = \\underset{a \\in A}{\\operatorname{argmax}} Q_{t}(a) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:10:40.465192Z",
     "start_time": "2024-11-18T21:10:40.461875Z"
    }
   },
   "outputs": [],
   "source": [
    "## The initial action value is set to be zero\n",
    "## There were no actions taken and thus every action value is 0. \n",
    "\n",
    "def init_Q(n_bandits):\n",
    "    \"\"\"Initialize the action-value function Q with zeros\"\"\"\n",
    "    Q = np.zeros(n_bandits)\n",
    "    return Q\n",
    "\n",
    "# Setting the initial action - values for the bandits to be 0 \n",
    "Q = init_Q(len(means)) \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:10:41.813222Z",
     "start_time": "2024-11-18T21:10:41.808314Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define the function that updates the given action value estimate with the new reward.\n",
    "\n",
    "def select_action(Q:list, epsilon: float = 1.0) -> int:\n",
    "    \"\"\"\n",
    "    Selects an action using the epsilon-greedy policy.\n",
    "\n",
    "    If espilon is 1, the action is selected randomly.\n",
    "    If epsilon is 0, the action is selected greedily.\n",
    "    \"\"\"\n",
    "    if (np.random.random() < epsilon) or (np.all(Q == 0)):\n",
    "        action = np.random.choice(range(len(Q)))\n",
    "    else:\n",
    "        action = np.argmax(Q)\n",
    "    return action\n",
    "\n",
    "# Defining the reward for each action \n",
    "rewards_generator = {\n",
    "    0: lambda: np.random.normal(means[0], 1),\n",
    "    1: lambda: np.random.normal(means[1], 1),\n",
    "    2: lambda: np.random.normal(means[2], 1),\n",
    "    3: lambda: np.random.normal(means[3], 1),\n",
    "    4: lambda: np.random.normal(means[4], 1)\n",
    "}\n",
    "\n",
    "# Defining the function that gets the reward based on the action\n",
    "def get_reward(action: int) -> float:\n",
    "    return rewards_generator[action]()\n",
    "\n",
    "def update_Q(Q:list, action: int, alpha: float = 0.1) -> list:\n",
    "    \"\"\"\n",
    "    Updates the action-value function using the sample-average method.\n",
    "    \"\"\"\n",
    "    Q[action] += alpha * (get_reward(action) - Q[action])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:10:47.977790Z",
     "start_time": "2024-11-18T21:10:47.973789Z"
    }
   },
   "outputs": [],
   "source": [
    "### one simulation and the result.\n",
    "\n",
    "# Selecting a random action\n",
    "action = select_action(Q, epsilon = 1.0) \n",
    "\n",
    "# Getting the rewward \n",
    "reward = get_reward(action)\n",
    "\n",
    "# Updating the action-value function\n",
    "Q = update_Q(Q, action)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################\n",
    "\n",
    "**Note that**\n",
    "- the agent has taken one action ant that action produced a certain reward. \n",
    "- If we instruct the agent to maximize the reward, he will make the same choice again. \n",
    "- We want the agent to learn more, since other bandits may be better than the one the agent already has seen. \n",
    "\n",
    "## Pure exploration algorithm\n",
    "\n",
    "- Define an algorithm where the bandit is choosed randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:11:06.103555Z",
     "start_time": "2024-11-18T21:11:06.042229Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Defining the number of actions \n",
    "n_actions = 1000\n",
    "rewards = []\n",
    "Q = init_Q(len(means))\n",
    "for action in range(n_actions):\n",
    "    # Selecting an action\n",
    "    action = select_action(Q, epsilon = 1.0) \n",
    "\n",
    "    # Getting the rewward \n",
    "    reward = get_reward(action)\n",
    "\n",
    "    # Appending to the list of rewards\n",
    "    rewards.append(reward)\n",
    "\n",
    "# Plotting the rewards\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(rewards)\n",
    "ax.set_title('Rewards')\n",
    "ax.set_xlabel('Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Epsilon greedy algorithm \n",
    "\n",
    "- What is `epsilon` parameter in the `select_action()` function?\n",
    "- This parameter is also known as the `exploration rate`. \n",
    "- For a subset of time steps, we say to our agent that instead of chosing the best action **seen so far**, he should explore and try out other actions. \n",
    "\n",
    "- Thus, by default, our agent choses the argument that maximizes the action value estimate: \n",
    "\n",
    "$$argmax_{a} Q_{t}(a)$$ \n",
    "\n",
    "- This is called the `greedy` action. \n",
    "- The $\\epsilon$ - greedy algorithm is a simple way to balance exploration and exploitation by alowing our agent to explore the space of actions with a probability $\\epsilon$.  \n",
    "\n",
    "- In the epsilon-greedy algorithm analysis, we tend to do the runs thousands of times and then average the rewards at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:11:49.662554Z",
     "start_time": "2024-11-18T21:11:12.021940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the lists of epsilon values\n",
    "epsilons = [0.0, 0.1, 0.4, 0.8, 1.0]\n",
    "\n",
    "# Defining the number of actions per run\n",
    "n_actions = 500\n",
    "\n",
    "# Defining the number of runs \n",
    "n_runs = 2000\n",
    "\n",
    "# Master dataframe \n",
    "d = pd.DataFrame()\n",
    "Q_df = pd.DataFrame()\n",
    "\n",
    "for epsilon in tqdm(epsilons):\n",
    "    runs = []\n",
    "    Q_run = []\n",
    "    for run in range(n_runs):\n",
    "        rewards = []\n",
    "        Q = init_Q(len(means))\n",
    "        for action in range(n_actions):\n",
    "            # Selecting an action\n",
    "            action = select_action(Q, epsilon = epsilon) \n",
    "\n",
    "            # Getting the rewward \n",
    "            reward = get_reward(action)\n",
    "\n",
    "            # Updating the action-value function\n",
    "            Q = update_Q(Q, action)\n",
    "\n",
    "            # Appending to the list of rewards\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # Appending the rewards to the runs\n",
    "        runs = np.append(runs, rewards)\n",
    "\n",
    "        # Appending the action-value function to the list of action-value functions\n",
    "        Q_run.append(Q)\n",
    "    \n",
    "    # Averaging the Q_run and appending to Q_eval \n",
    "    Q_run = np.mean(Q_run, axis=0)\n",
    "\n",
    "    # Converting to dataframe \n",
    "    Q_run = pd.DataFrame(Q_run, columns=['Q'])\n",
    "    Q_run['bandit'] = Q_run.index\n",
    "    Q_run['epsilon'] = epsilon\n",
    "\n",
    "    Q_df = pd.concat([Q_df, Q_run])\n",
    "\n",
    "    # Averaging \n",
    "    runs = runs.reshape(n_runs, n_actions)\n",
    "    runs = runs.mean(axis=0)\n",
    "\n",
    "    df = pd.DataFrame(runs, columns=['reward'])\n",
    "    df['epsilon'] = epsilon\n",
    "    df['step'] = df.index\n",
    "\n",
    "    d = pd.concat([d, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:11:57.433527Z",
     "start_time": "2024-11-18T21:11:57.342720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the rewards \n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.lineplot(x='step', y='reward', hue='epsilon', data=d, ax=ax)\n",
    "ax.set_title('Average reward at each step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T21:12:01.463834Z",
     "start_time": "2024-11-18T21:12:01.352484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ploting a barplot of the evaluated q values\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.barplot(x='bandit', y='Q', hue='epsilon', data=Q_df, ax=ax, edgecolor='black', linewidth=1)\n",
    "ax.set_title('Average Q values')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
