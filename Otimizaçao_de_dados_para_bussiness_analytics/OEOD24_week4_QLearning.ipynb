{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94e8838",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    " <div class=\"alert alert-block alert-info\"><b>Master in Data Science - Iscte <b>\n",
    "     </div>\n",
    "</font> \n",
    " \n",
    " \n",
    "     \n",
    "    \n",
    "  <font size=\"5\"> OEOD </font>\n",
    "  \n",
    "  \n",
    "  \n",
    "  <font size=\"3\"> **Diana Aldea Mendes**, November 2024 </font>\n",
    "  \n",
    "   \n",
    "  <font size=\"3\"> *diana.mendes@iscte-iul.pt* </font> \n",
    "  \n",
    "    \n",
    " \n",
    "  \n",
    "    \n",
    "  <font color='blue'><font size=\"5\"> <b>Week 4 - Q-learning<b></font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23843e",
   "metadata": {},
   "source": [
    "_________________\n",
    "\n",
    "# Q-learning algorithm\n",
    "\n",
    "- Implementation of the Q-learning algorithm for a grid-world context\n",
    "- Gridworld problem involves an agent navigating a grid to reach a goal while avoiding obstacles\n",
    "\n",
    "## Environment\n",
    "\n",
    "- We define a 5x5 grid with the following properties:\n",
    "\n",
    "    - The agent starts at the top-left corner (0, 0).\n",
    "    - The goal is at the bottom-right corner (4, 4) with a high positive reward of +10.\n",
    "    - Obstacles, with negative rewards (-1), are placed at grid points (1, 1) and (2, 3).\n",
    "    - The rest of the grid is empty and can either have neutral (0) or small negative rewards (e.g., -0.1, but in this post we consider only the neutral case), encouraging the agent to reach the goal efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecce94b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:28:09.254250Z",
     "start_time": "2024-11-05T18:28:09.035662Z"
    }
   },
   "outputs": [],
   "source": [
    "### define the grid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "grid = np.zeros((5, 5))\n",
    "grid[-1, -1] = 10  # Goal with a high positive reward\n",
    "grid[1, 1] = -1  # Obstacle with a negative reward\n",
    "grid[2, 3] = -1  # Another obstacle\n",
    "print(grid)\n",
    "\n",
    "\n",
    "### reward of +10 when reaching the goal, -1 for hitting obstacles, and 0 for navigating other empty cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb736e",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "- The essence of Q-Learning lies in the Q-table, a data structure that stores the estimated values (Q-values) of taking different actions in different states.These Q-values represent the expected future rewards for each action-state pair. \n",
    "- The algorithm follows these key steps:\n",
    "1. Initialization: The Q-table is initialized with arbitrary values, often zeros.\n",
    "2. Exploration and Exploitation: The agent interacts with the environment,taking actions based on the Q-table. It balances exploration (trying random actions to gather information) with exploitation (choosing actions with the highest Q-values to maximize rewards).\n",
    "3. Action Selection: The agent selects an action based on the Q-values in the current state. It might choose the action with the highest Q-value (greedy approach) or introduce some randomness for exploration (epsilon-greedy approach).\n",
    "4. Observation and Reward: The agent observes the next state and receives a reward based on the action taken.\n",
    "5. Q-Value Update: The Q-value for the previous state-action pair is updated using the Bellman equation. The equation combines the immediate reward with the discounted maximum expected future reward from thenext state.\n",
    "6. Iteration: Steps 2–5 are repeated until the Q-table converges, meaning theQ-values stabilize, indicating the agent has learned the optimal policy.\n",
    "\n",
    "- Write the Q-learning formula as:\n",
    "\n",
    "$$\n",
    "Q(s, a)=(1-\\alpha) Q(s, a)+\\alpha\\left(R+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)\\right)\n",
    "$$\n",
    "\n",
    "- It can be interpreted as:\n",
    "\n",
    "    - $(1 — α) Q(s, a)$: The agent’s expected reward based on old knowledge.\n",
    "    - $α R$: The new reward based on recent exploration.\n",
    "    - $γ max Q(s’, a’)$: The maximum expected future reward, weighted by $γ$, the discount factor.\n",
    "\n",
    "- This balance between old knowledge, new experience, and future reward drives the agent to learn the best policy over time.\n",
    "\n",
    "\n",
    "_____________________\n",
    "\n",
    "- Explore vs. Exploit\n",
    "    - If we let the agent always randomly choose an action, it could eventually learn the Q-table, but the process will never be efficient. On the contrary, if we only choose an action based on the maximization of the Q-value, the agent will tend to always take the same route, overfitting the current environment setup. Furthermore it will suffer from great variance as it will not be able to find the proper route in another environment setup.\n",
    "\n",
    "    - To prevent those two scenarios to occur and to try to find a trade-off, we add another hyperparameter epsilon ϵ, which is the probability with which we choose a random action instead of the one computed from the Q-table. Playing with this parameter allows us to find an equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33032f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:28:43.427618Z",
     "start_time": "2024-11-05T18:28:11.551916Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "grid_size = 5\n",
    "gamma = 0.1  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000\n",
    "\n",
    "grid = np.zeros((5, 5))\n",
    "grid[-1, -1] = 10  # Goal with a high positive reward\n",
    "grid[1, 1] = -1  # Obstacle with a negative reward\n",
    "grid[2, 3] = -1  # Another obstacle\n",
    "\n",
    "# Action space: up, down, left, right\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Define the transition function\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up' and i > 0:\n",
    "        return (i - 1, j)\n",
    "    elif action == 'down' and i < grid_size - 1:\n",
    "        return (i + 1, j)\n",
    "    elif action == 'left' and j > 0:\n",
    "        return (i, j - 1)\n",
    "    elif action == 'right' and j < grid_size - 1:\n",
    "        return (i, j + 1)\n",
    "    return state\n",
    "\n",
    "# Define the reward function\n",
    "def get_reward(state):\n",
    "    return grid[state]\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return actions[np.argmax(q_table[state[0], state[1], :])]\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start at top-left corner\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "        action_index = actions.index(action)\n",
    "\n",
    "        # Update Q-table using Q-learning update rule\n",
    "        q_table[state[0], state[1], action_index] += alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1], :]) - q_table[state[0], state[1], action_index])\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # End episode if we reach the goal\n",
    "        if reward == 10:\n",
    "            done = True\n",
    "\n",
    "# Display learned Q-values\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2827a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6738395",
   "metadata": {},
   "source": [
    "_____________________\n",
    "\n",
    "\n",
    "# Taxi Problem - Gym\n",
    "\n",
    "- Solve Taxi Problem by using Q-learning Algorithm\n",
    "- Taxi problem: an agent must navigate a grid to transport passengers to their destinations.\n",
    "- Grid problem, grid size = 5x5\n",
    "- The environment has several fixed locations (denoted by the letters R, Y, G, and B for Red, Yellow, Green, and Blue). \n",
    "- These letters represent the possible pick-up and drop-off locations for passengers.\n",
    "- The taxi starts in a random position, and at each episode, the passenger’s location and destination are randomly assigned to one of these four spots. \n",
    "- The agent (taxi) must figure out the best sequence of moves to:\n",
    "\n",
    "    - Navigate to the passenger’s location,\n",
    "    - Pick up the passenger,\n",
    "    - Navigate to the dropoff location,\n",
    "    - Drop off the passenger successfully.\n",
    "    \n",
    "- The goal of the agent is to maximize the reward by performing the correct sequence of actions in the minimum number of steps.\n",
    "\n",
    "_________________\n",
    "\n",
    "- Two main steps in the process\n",
    "1. Training the agent using Q-learning\n",
    "2. Testing the trained agent by visualizing its behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369f773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:32:29.691226Z",
     "start_time": "2024-11-05T18:32:29.686037Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### import libraries\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "### import libraries\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import random\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f44017",
   "metadata": {},
   "source": [
    "## Step 1: Q-learning\n",
    "\n",
    "- The Environment: We initialize the Taxi-v3 environment, which provides a grid with fixed pickup and dropoff points.\n",
    "- Q-Table Initialization: We initialize a Q-table (a 2D array) with dimensions corresponding to the number of states and actions in the environment. This table will store the learned action values for each state.\n",
    "- Hyperparameters: learning rate (learning_rate), discount rate (discount_rate), and exploration-exploitation tradeoff (epsilon) \n",
    "- Episodes and Steps: The training runs over a set number of episodes, where each episode represents an attempt by the agent to solve the taxi problem from a new random initial state.\n",
    "- Epsilon Decay: Epsilon governs the agent’s exploration-exploitation balance. Over time, we decay epsilon using an inverse decay rule to slowly shift from exploring to exploiting the learned strategy as training progresses.\n",
    "\n",
    "____________________\n",
    "\n",
    "- The Q-learning algorithm iteratively updates the Q-table based on the rewards received after each action. Over time, the agent learns which actions yield the best rewards in different states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5a3c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:29:12.533250Z",
     "start_time": "2024-11-05T18:29:12.507309Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create Taxi environment\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[1;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxi-v3\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mansi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# create Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d3784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:29:13.613334Z",
     "start_time": "2024-11-05T18:29:13.603836Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize q-table\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "qtable = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292c8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:29:14.524534Z",
     "start_time": "2024-11-05T18:29:14.515485Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.9\n",
    "discount_rate = 0.8\n",
    "epsilon = 0.2\n",
    "decay_rate = 0.005\n",
    "\n",
    "# training variables\n",
    "num_episodes = 2000\n",
    "max_steps = 99 # per episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aded733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:30:36.275983Z",
     "start_time": "2024-11-05T18:30:35.187397Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Training the agent...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Exploration-exploitation tradeoff\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(qtable[state,:])  # Exploit\n",
    "\n",
    "        # Take an action and observe the reward\n",
    "        output = env.step(action)\n",
    "        new_state, reward, done, info = output[0], output[1], output[2], output[3]\n",
    "\n",
    "        # Q-learning update\n",
    "        qtable[state,action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
    "\n",
    "        # Update state\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = 1.0 / (1.0 + decay_rate * episode)\n",
    "\n",
    "print(f\"Training completed after {num_episodes} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e46fe",
   "metadata": {},
   "source": [
    "## Testing step\n",
    "- When training is complete, visualize the agent’s behavior by letting it solve a few episodes of the Taxi problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f00958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T18:33:22.109179Z",
     "start_time": "2024-11-05T18:32:33.946249Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize the trained agent\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m visualize_agent(\u001b[43menv\u001b[49m, qtable, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_agent(env, qtable, episodes=5, max_steps=100):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        print(f\"Episode {episode + 1}\\n\")\n",
    "        sleep(1)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            sleep(0.5)  # Adjust speed of animation\n",
    "\n",
    "            # Choose action based on Q-table\n",
    "            action = np.argmax(qtable[state, :])\n",
    "            output = env.step(action)\n",
    "            new_state, reward, done, info = output[0], output[1], output[2], output[3]            \n",
    "            \n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode finished after {step + 1} timesteps\\n\")\n",
    "                sleep(2)\n",
    "                clear_output(wait=True)\n",
    "                break\n",
    "\n",
    "# Visualize the trained agent\n",
    "visualize_agent(env, qtable, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbb0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
