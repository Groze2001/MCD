{"cells":[{"cell_type":"markdown","source":["<font size=\"5\">\n"," <div class=\"alert alert-block alert-info\"> <b>Week 7: Deep Neural Networks (DNNs)<b>\n","     </div>\n","</font>\n","\n","      \n","    \n","  <font size=\"4\"> MCD - Análise de Séries Temporais e Previsão </font>\n","  \n","  \n","  <font size=\"3\"> **Filipe R. Ramos** </font>\n","      \n","   \n","  <font size=\"3\"> *frjrs@iscte-iul.pt* </font>  \n","   \n","    \n","  <font size=\"3\">ISCTE-IUL </font>"],"metadata":{"id":"_5HidhS9KQgu"},"id":"_5HidhS9KQgu"},{"cell_type":"markdown","source":["<font size=\"5\">\n","    <div class=\"alert alert-warning\" role=\"alert\"> -- Deep Neural Networks -- </div> </font>\n","\n","## **Agenda**\n","\n","1. Deep Neural Networks (DNNs): Introduction\n","\n","2. Recurrent Neural Networks (RNNs)\n","\n","3. **Long Short-Term Memory** (LSTM)\n","---------------------\n","\n","\n","## **References**\n","- [Chollet, F. (2017)](https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf)\n","\n","- [Haykin, S. (2009)](https://lps.ufrj.br/~caloba/Livros/Haykin2009.pdf)\n","\n","- [Introduction to Machine Learning with Python  book](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/)\n"],"metadata":{"id":"SkrP_DZnKUVc"},"id":"SkrP_DZnKUVc"},{"cell_type":"markdown","source":["## **Exercício 2: Overfitting e Cross-validation com Séries Temporais**\n","\n","-----------------\n","**OBJETIVO:** Explorar o conceito de overfitting e como ele se apresenta numa rede LSTM aplicada a séries temporais (compreender como um modelo se pode ajustar \"excessivamente\" ao conjunto de treino e não generalizar). Para tal, será utilizada uma técnica de validação cruzada específica para séries temporais, o `TimeSeriesSplit`, que respeita a sequência temporal dos dados.\n","\n","[**Porque não podemos usar k-fold?**]\n","\n","----------------------\n","\n","**1. Dividir os dados em treino e validação:**\n","\n","- **Objetivo:** manter a sequência temporal e permitir uma validação mais realista em previsões futuras.\n","- **Instruções:** usar `TimeSeriesSplit` do `sklearn.model_selection`, que divide os dados em múltiplas janelas de treino e validação sem os baralhar. Em cada \"fold\", uma nova parte inicial é usada para treino, e a próxima sequência para validação.\n","\n","**2. Avaliar o modelo na validação em cada época:**\n","\n","- **Objetivo:** observar o erro de validação ao longo das épocas para identificar sinais de overfitting.\n","- **Instruções:** modificar o código de `fit` adicionando `validation_data=(X_val, y_val)`. Isso permitirá acompanhar como o erro de validação pode divergir do erro de treino, indicando se o modelo está a \"memorizar\" os dados de treino em vez de aprender padrões úteis.\n","\n","**3. Implementar validação cruzada sequencial com TimeSeriesSplit:**\n","\n","- **Objetivo:** Avaliar a consistência do modelo em diferentes janelas temporais, observando o comportamento em múltiplos \"folds\".\n","- **Instruções:** implementar o `TimeSeriesSplit` no código. Para cada janela de treino, o modelo será ajustado com base nos dados iniciais e validado com base nos dados sequenciais subsequentes. Deve observar-se como o erro de validação varia entre os \"folds\" e interpretar se o modelo consegue generalizar.\n","\n","\n","\n","**NOTA:**\n","**Métricas de Avaliação:**\n","Utilizar o MAPE (Mean Absolute Percentage Error) para cada \"fold\", uma métrica que indica a precisão das previsões. Para cada conjunto de validação, calcular o MAPE e observar o desempenho geral do modelo."],"metadata":{"id":"2IF82IWRkrTc"},"id":"2IF82IWRkrTc"},{"cell_type":"markdown","id":"6e861697","metadata":{"id":"6e861697"},"source":["# **Import libraries**"]},{"cell_type":"code","execution_count":null,"id":"e3ea32c2","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:37:20.074716Z","start_time":"2024-10-27T22:37:20.071865Z"},"id":"e3ea32c2"},"outputs":[],"source":["### bibliotecas\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import yfinance as yf\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import mean_absolute_percentage_error\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","from keras.optimizers import Adam\n","from keras.metrics import MeanSquaredError"]},{"cell_type":"markdown","source":["# **Import Data**"],"metadata":{"id":"UAgRREq9MyWN"},"id":"UAgRREq9MyWN"},{"cell_type":"code","execution_count":null,"id":"206cbbfe","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:43:17.847712Z","start_time":"2024-10-27T22:43:17.664011Z"},"id":"206cbbfe"},"outputs":[],"source":["### importar dados do yahoo finance\n","\n","\n","AMZN = yf.download('AMZN',\n","                      start='2013-01-01',\n","                      end='2024-09-30',\n","                      progress=False)\n","\n","# AMZN = yf.download('AMZN') for all\n","# Check the columns of the AMZN DataFrame\n","print(AMZN.columns)\n","\n","all_data = AMZN[['Open', 'High', 'Low', 'Close','Volume']].round(2)\n","all_data.head(10)\n","print(\"There are \"+ str(all_data[:'2023'].shape[0]) + \" observations in the training data\")\n","print(\"There are \"+ str(all_data['2024':].shape[0]) + \" observations in the test data\")\n","all_data['Close'].plot()\n","\n","\n"]},{"cell_type":"markdown","source":["# **Data Preparation**"],"metadata":{"id":"ypIF9GDKOIdu"},"id":"ypIF9GDKOIdu"},{"cell_type":"code","execution_count":null,"id":"ab1bf561","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:44:25.000012Z","start_time":"2024-10-27T22:44:24.983606Z"},"id":"ab1bf561"},"outputs":[],"source":["### função que faz a divisão em treino-teste, normaliza os dados\n","### transforma a séries temporal (e os lags) numa dataframe\n","\n","\n","def ts_train_test_normalize(all_data,time_steps,for_periods):\n","    '''\n","    input:\n","      data: dataframe with dates and price data\n","    output:\n","      X_train, y_train: data from 2013/1/1-2023/12/31\n","      X_test:  data from 2024 -\n","      sc:      insantiated MinMaxScaler object fit to the training data\n","    '''\n","    # create training and test set\n","    ts_train = all_data[:'2023'].iloc[:,0:1].values\n","    ts_test  = all_data['2024':].iloc[:,0:1].values\n","    ts_train_len = len(ts_train)\n","    ts_test_len = len(ts_test)\n","\n","    # scale the data\n","    from sklearn.preprocessing import MinMaxScaler\n","    sc = MinMaxScaler(feature_range=(0,1))\n","    ts_train_scaled = sc.fit_transform(ts_train)\n","\n","    # create training data of s samples and t time steps\n","    X_train = []\n","    y_train = []\n","    y_train_stacked = []\n","    for i in range(time_steps,ts_train_len-1):\n","        X_train.append(ts_train_scaled[i-time_steps:i,0])\n","        y_train.append(ts_train_scaled[i:i+for_periods,0])\n","    X_train, y_train = np.array(X_train), np.array(y_train)\n","\n","    # Reshaping X_train for efficient modelling\n","    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n","\n","    inputs = pd.concat((all_data[\"Close\"][:'2023'], all_data[\"Close\"]['2024':]),axis=0).values\n","    inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n","    inputs = inputs.reshape(-1,1)\n","    inputs  = sc.transform(inputs)\n","\n","    # Preparing X_test\n","    X_test = []\n","    for i in range(time_steps,ts_test_len+time_steps-for_periods):\n","        X_test.append(inputs[i-time_steps:i,0])\n","\n","    X_test = np.array(X_test)\n","    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n","\n","    return X_train, y_train , X_test, sc\n","\n","X_train, y_train, X_test, sc = ts_train_test_normalize(all_data,5,2)\n","X_train.shape[0],X_train.shape[1]"]},{"cell_type":"code","execution_count":null,"id":"9e3129bf","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:45:00.236013Z","start_time":"2024-10-27T22:45:00.232025Z"},"id":"9e3129bf"},"outputs":[],"source":["### função para representar graficamente os valores preditos vs teste\n","\n","\n","def actual_pred_plot(preds):\n","    #Plot the actual vs. prediction\n","    actual_pred = pd.DataFrame(columns = ['Close', 'prediction'])\n","    actual_pred['Close'] = all_data.loc['2024':,'Close'][0:len(preds)]\n","    actual_pred['prediction'] = preds[:,0]\n","\n","    from keras.metrics import MeanSquaredError\n","    m = MeanSquaredError()\n","    m.update_state(np.array(actual_pred['Close']),np.array(actual_pred['prediction']))\n","\n","    return (m.result().numpy(), actual_pred.plot() )"]},{"cell_type":"code","execution_count":null,"id":"110f8105","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:41:25.808468Z","start_time":"2024-10-27T22:41:25.801469Z"},"id":"110f8105"},"outputs":[],"source":["# # Convert the 3-D shape of X_train to a data frame so we can see\n","X_train_see = pd.DataFrame(np.reshape(X_train, (X_train.shape[0],X_train.shape[1])))\n","y_train_see = pd.DataFrame(y_train)\n","pd.concat([X_train_see,y_train_see],axis=1)\n","\n","# # Convert the 3-D shape of X_test to a data frame so we can see:\n","X_test_see = pd.DataFrame(np.reshape(X_test, (X_test.shape[0],X_test.shape[1])))\n","pd.DataFrame(X_test_see)\n","\n","print(\"There are \" + str(X_train.shape[0]) + \" samples in the training data\")\n","print(\"There are \" + str(X_test.shape[0]) + \" samples in the test data\")"]},{"cell_type":"markdown","source":["# **LSTM Model - Forecasting**"],"metadata":{"id":"5z27bRZH_yUt"},"id":"5z27bRZH_yUt"},{"cell_type":"code","execution_count":null,"id":"f358f572","metadata":{"ExecuteTime":{"end_time":"2024-10-27T22:50:14.043102Z","start_time":"2024-10-27T22:50:04.408740Z"},"id":"f358f572"},"outputs":[],"source":["### função para LSTM\n","\n","\n","def LSTM_model(X_train, y_train, X_test, sc):\n","    # create a model\n","    from keras.models import Sequential\n","    from keras.layers import Dense, SimpleRNN, GRU, LSTM\n","    from keras.optimizers import SGD, Adam\n","\n","    # The LSTM architecture\n","    my_LSTM_model = Sequential()\n","    my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n","    #my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n","    #my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n","    my_LSTM_model.add(LSTM(units=50, activation='tanh')) #Como é a última camada LSTM, a rede não precisa mais de uma sequência de saídas, apenas a saída final.\n","    # Change the Dense layer to predict multiple outputs\n","    my_LSTM_model.add(Dense(units=y_train.shape[1])) # Adjust the units to match y_train's output shape # Changed 'model' to 'my_LSTM_model'\n","\n","\n","    # Compiling\n","    my_LSTM_model.compile(optimizer=Adam(learning_rate=0.01),loss='mean_squared_error')\n","    # Fitting to the training set\n","    my_LSTM_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=0)\n","\n","    LSTM_prediction = my_LSTM_model.predict(X_test)\n","    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n","\n","    return my_LSTM_model, LSTM_prediction\n","\n","X_train, y_train, X_test, sc = ts_train_test_normalize(all_data,5,2)\n","my_LSTM_model, LSTM_prediction = LSTM_model(X_train, y_train, X_test, sc)\n","LSTM_prediction[1:10]\n","actual_pred_plot(LSTM_prediction)\n","\n","\n","#Pedir, caso queiram, o vetor das previsões\n","\n","#print(LSTM_prediction)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e8a335b8","metadata":{"id":"e8a335b8"},"outputs":[],"source":["### Calcular métricas de erro relativas à previsão\n","\n","\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n","\n","y_test = all_data.loc['2024':,'Close'][0:len(LSTM_prediction)].values\n","# Select the first column (predicted 'Close' values) from LSTM_prediction\n","LSTM_prediction_close = LSTM_prediction[:, 0]\n","mae = mean_absolute_error(y_test, LSTM_prediction_close)\n","print(\"Mean Absolute Error (MAE):\", mae)\n","\n","mape = mean_absolute_percentage_error(y_test, LSTM_prediction_close)\n","print(f\"Mean Absolute Percentage Error (MAPE): {mape * 100:.3f}%\")"]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[{"file_id":"1K5d0uUyrEWL7Kd36H4i8TnBexcFpbTeM","timestamp":1743980152357},{"file_id":"1PoKEgr_M-MFlAieiJQ79BWibumTXmPi9","timestamp":1743979539947},{"file_id":"1nTp7dsMoLLQNps1nVYgt9PYC20vL-unI","timestamp":1730245306193},{"file_id":"112LDK1eqwNEtWTeiFWq41upNTGJsChMn","timestamp":1730236801068}]}},"nbformat":4,"nbformat_minor":5}