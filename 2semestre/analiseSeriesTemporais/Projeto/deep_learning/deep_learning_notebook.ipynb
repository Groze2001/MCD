{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import optuna\n",
    "\n",
    "# Type hinting for better code documentation\n",
    "from typing import Tuple, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data: np.ndarray, timesteps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sequences of data for time series forecasting.\n",
    "\n",
    "    This function takes a time series dataset and splits it into sequences of a specified length (timesteps).\n",
    "    Each sequence is used as input (X), and the value immediately preceding the sequence is used as the target (y).\n",
    "    \n",
    "    Note: This creates a look-ahead prediction approach - we use n timesteps to predict the value\n",
    "    that came right before those timesteps. This is common in certain types of time series models.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The time series data to be split into sequences.\n",
    "    - timesteps (int): The number of time steps in each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - X (numpy.ndarray): Array of input sequences, where each sequence has a shape of (timesteps, features).\n",
    "    - y (numpy.ndarray): Array of target values corresponding to each sequence.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    # Calculate how many complete sequences can be extracted from the data\n",
    "    num_samples = len(data) // timesteps  # Ensure only full sequences are used\n",
    "\n",
    "    # Iterate backwards through the dataset to create sequences\n",
    "    for i in range(num_samples):\n",
    "        # Calculate start and end indices for this sequence\n",
    "        start = len(data) - (i + 1) * timesteps\n",
    "        end = start + timesteps\n",
    "\n",
    "        if start >= 0:  # Ensure valid indexing (there's at least one value before the sequence)\n",
    "            X.append(data[start:end])           # Sequence becomes input feature\n",
    "            y.append(data[start - 1])           # Value before sequence becomes target\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def generate_model_checkpoint(model_weights_location: str) -> keras.callbacks.ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Generate a ModelCheckpoint callback for saving the best model weights.\n",
    "    \n",
    "    ModelCheckpoint is a crucial callback that prevents overfitting by saving weights\n",
    "    only when the model improves on the validation set.\n",
    "\n",
    "    This function creates a Keras ModelCheckpoint callback that monitors the validation loss\n",
    "    during training and saves the model weights whenever the validation loss improves.\n",
    "\n",
    "    Parameters:\n",
    "    - model_weights_location (str): The file path where the model weights will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - keras.callbacks.ModelCheckpoint: A ModelCheckpoint callback configured to save the best weights.\n",
    "    \"\"\"\n",
    "    return keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_weights_location,\n",
    "        monitor='val_loss',            # Monitor validation loss\n",
    "        save_best_only=True,           # Only save when val_loss improves\n",
    "        save_weights_only=True,        # Save only weights, not entire model\n",
    "        mode='min'                     # Lower validation loss is better\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_early_stop() -> keras.callbacks.EarlyStopping:\n",
    "    \"\"\"\n",
    "    Generate an EarlyStopping callback for training.\n",
    "    \n",
    "    Early stopping prevents overfitting by halting training when performance\n",
    "    on validation data stops improving.\n",
    "\n",
    "    This function creates a Keras EarlyStopping callback that monitors the validation loss\n",
    "    during training and stops the training process if the validation loss does not improve\n",
    "    for a specified number of epochs (patience). It also restores the model weights to the\n",
    "    best weights observed during training.\n",
    "\n",
    "    Returns:\n",
    "    - keras.callbacks.EarlyStopping: An EarlyStopping callback configured to monitor validation loss.\n",
    "    \"\"\"\n",
    "    return keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',           # Monitor validation loss\n",
    "        patience=5,                   # Wait for 5 epochs of no improvement before stopping\n",
    "        restore_best_weights=True     # Restore to the best weights found during training\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control flags for different phases of the workflow\n",
    "IS_TO_FINE_TUNE = False\n",
    "IS_TO_BENCHMARK = False\n",
    "\n",
    "# Model configuration parameters\n",
    "SEQ_LENGTH = 1\n",
    "SEED = 64\n",
    "\n",
    "# Model weight initialization strategies (for reproducibility)\n",
    "KERNEL_INITIALIZER = keras.initializers.GlorotUniform(seed=SEED)\n",
    "RECURRENT_INITIALIZER = keras.initializers.Orthogonal(seed=SEED)\n",
    "\n",
    "# File paths\n",
    "PATH = \"../dataset\"\n",
    "DATA_FILENAME = f\"{PATH}/data.csv\"\n",
    "\n",
    "# Checkpoint callbacks to save the best model weights during training\n",
    "RENEWABLES_CHECKPOINT = generate_model_checkpoint(f\"{PATH}/best_renewables_model.weights.h5\")\n",
    "NON_RENEWABLES_CHECKPOINT = generate_model_checkpoint(f\"{PATH}/best_non_renewables_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file\n",
    "df = pd.read_csv(DATA_FILENAME)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert the timestamp column to datetime format and set it as the index\n",
    "df['Data e Hora'] = pd.to_datetime(df['Data e Hora'])\n",
    "df = df.set_index('Data e Hora')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target columns for renewable and non-renewable energy production\n",
    "renewables_df = df[\"producao_renovavel\"]\n",
    "non_renewables_df = df[\"producao_nao_renovavel\"]\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the split points in terms of number of samples\n",
    "train_split = int(df.shape[0] * train_ratio * (1 - val_ratio))\n",
    "val_split = int(df.shape[0] * train_ratio * val_ratio)\n",
    "test_split = int(df.shape[0] * test_ratio)\n",
    "\n",
    "# Split the renewable energy production dataset\n",
    "renewables_df_train = renewables_df[:train_split]\n",
    "renewables_df_val = renewables_df[train_split:train_split + val_split]\n",
    "renewables_df_test = renewables_df[:test_split]\n",
    "\n",
    "# Split the non-renewable energy production dataset\n",
    "non_renewables_df_train = non_renewables_df[:train_split]\n",
    "non_renewables_df_val = non_renewables_df[train_split:train_split + val_split]\n",
    "non_renewables_df_test = non_renewables_df[:test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling is important for neural networks to ensure stable and fast convergence\n",
    "renewables_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "non_renewables_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the renewable energy datasets\n",
    "renewables_df_train_scaled = renewables_scaler.fit_transform(renewables_df_train.values.reshape(-1, 1))\n",
    "renewables_df_val_scaled = renewables_scaler.transform(renewables_df_val.values.reshape(-1, 1))\n",
    "renewables_df_test_scaled = renewables_scaler.transform(renewables_df_test.values.reshape(-1, 1))\n",
    "\n",
    "# Scale the non-renewable energy datasets\n",
    "non_renewables_df_train_scaled = non_renewables_scaler.fit_transform(non_renewables_df_train.values.reshape(-1, 1))\n",
    "non_renewables_df_val_scaled = non_renewables_scaler.transform(non_renewables_df_val.values.reshape(-1, 1))\n",
    "non_renewables_df_test_scaled = non_renewables_scaler.transform(non_renewables_df_test.values.reshape(-1, 1))\n",
    "\n",
    "# Create sequence datasets for time series modeling using the function defined earlier\n",
    "# For renewable energy data:\n",
    "renewables_X_train, renewables_y_train = create_sequences(renewables_df_train_scaled, SEQ_LENGTH)\n",
    "renewables_X_val, renewables_y_val = create_sequences(renewables_df_val_scaled, SEQ_LENGTH)\n",
    "renewables_X_test, renewables_y_test = create_sequences(renewables_df_test_scaled, SEQ_LENGTH)\n",
    "\n",
    "# For non-renewable energy data:\n",
    "non_renewables_X_train, non_renewables_y_train = create_sequences(non_renewables_df_train_scaled, SEQ_LENGTH)\n",
    "non_renewables_X_val, non_renewables_y_val = create_sequences(non_renewables_df_val_scaled, SEQ_LENGTH)\n",
    "non_renewables_X_test, non_renewables_y_test = create_sequences(non_renewables_df_test_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Print dataset sizes to verify the splits\n",
    "print(\"Renewable energy production time series split:\")\n",
    "print(\"Training set size:\", renewables_X_train.shape)\n",
    "print(\"Validation set size:\", renewables_X_val.shape)\n",
    "print(\"Test set size:\", renewables_X_test.shape)\n",
    "\n",
    "print()\n",
    "print(\"Non-renewable energy production time series split:\")\n",
    "print(\"Training set size:\", non_renewables_X_train.shape)\n",
    "print(\"Validation set size:\", non_renewables_X_val.shape)\n",
    "print(\"Test set size:\", non_renewables_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_builder(\n",
    "        input_shape: Tuple[int, int, int],\n",
    "        first_lstm_layer_size: int,\n",
    "        second_lstm_layer_size: int,\n",
    "        dense_layer_size: int,\n",
    "        dense_layer_activation_func: str = \"relu\",\n",
    "        kernel_initializer: Union[str, keras.initializers.GlorotUniform] = \"glorot_uniform\",\n",
    "        recurrent_initializer: Union[str, keras.initializers.Orthogonal] = \"orthogonal\"\n",
    "    ) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Build a stacked LSTM model for time series regression.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input layer\n",
    "    2. First LSTM layer with return_sequences=True to pass all outputs to next LSTM\n",
    "    3. Second LSTM layer that processes outputs from first LSTM\n",
    "    4. Dense layer with activation function (typically ReLU)\n",
    "    5. Output layer (single neuron for regression)\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: Shape of input data (samples, timesteps, features)\n",
    "    - first_lstm_layer_size: Number of units in first LSTM layer\n",
    "    - second_lstm_layer_size: Number of units in second LSTM layer\n",
    "    - dense_layer_size: Number of units in dense layer\n",
    "    - dense_layer_activation_func: Activation function for dense layer\n",
    "    - kernel_initializer: Weight initialization for kernels\n",
    "    - recurrent_initializer: Weight initialization for recurrent connections\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "    return keras.Sequential([\n",
    "        # Input layer defines the shape of input data\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First LSTM layer processes the input sequence and outputs a sequence\n",
    "        keras.layers.LSTM(first_lstm_layer_size, return_sequences=True, \n",
    "                          kernel_initializer=kernel_initializer, \n",
    "                          recurrent_initializer=recurrent_initializer, \n",
    "                          name=\"first_recurrent_layer\"),\n",
    "        \n",
    "        # Second LSTM layer processes the sequence from first LSTM and outputs a single vector\n",
    "        keras.layers.LSTM(second_lstm_layer_size, \n",
    "                          kernel_initializer=kernel_initializer, \n",
    "                          recurrent_initializer=recurrent_initializer, \n",
    "                          name=\"second_recurrent_layer\"),\n",
    "        \n",
    "        # Dense layer for further processing and introducing non-linearity\n",
    "        keras.layers.Dense(dense_layer_size, \n",
    "                          activation=dense_layer_activation_func, \n",
    "                          kernel_initializer=kernel_initializer, \n",
    "                          name=\"dense_layer\"),\n",
    "        \n",
    "        # Output layer - single neuron for regression with no activation (linear)\n",
    "        keras.layers.Dense(1, kernel_initializer=kernel_initializer, name=\"output_layer\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "Next is implemented a fine-tuning process for the LSTM-based model designed to forecast renewable and non-renewable energy production. The fine-tuning is conducted using Optuna, a state-of-the-art hyperparameter optimization framework. The workflow includes the following steps:\n",
    "1. **Objective Function Definition**: An objective function is defined to build, train, and evaluate models based on hyperparameters suggested by Optuna.\n",
    "2. **Hyperparameter Optimization**: Multiple trials are executed to identify the optimal combination of hyperparameters, such as LSTM layer sizes, dense layer size, and learning rate.\n",
    "3. **Model Training**: A final model is trained using the best hyperparameters, and the best weights are saved for future use.\n",
    "4. **Performance Evaluation**: Predictions are made on the test set, and the model's performance is assessed using metrics such as Mean Absolute Percentage Error (MAPE) and R-squared (R²).\n",
    "5. **Hyperparameter Persistence**: The best hyperparameters are saved to a JSON file for reproducibility and future reference.\n",
    "\n",
    "If fine-tuning is disabled (`IS_TO_FINE_TUNE = False`), the cell instead loads previously saved hyperparameters from a JSON file to avoid redundant optimization.\n",
    "\n",
    "### Renewables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TO_FINE_TUNE:\n",
    "    print(f\"Fine tunning 'renewable energy production' time series\")\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna hyperparameter optimization.\n",
    "        \n",
    "        This function:\n",
    "        1. Suggests hyperparameter values to try\n",
    "        2. Builds a model with those hyperparameters\n",
    "        3. Trains and evaluates the model\n",
    "        4. Returns the validation loss for Optuna to minimize\n",
    "        \n",
    "        Parameters:\n",
    "        - trial: Optuna trial object that manages hyperparameter suggestions\n",
    "        \n",
    "        Returns:\n",
    "        - Validation loss (to be minimized)\n",
    "        \"\"\"\n",
    "        # Suggest hyperparameters within defined ranges\n",
    "        first_lstm_layer_size = trial.suggest_int('first_lstm_layer_size', 32, 256, step=32)\n",
    "        second_lstm_layer_size = trial.suggest_int('second_lstm_layer_size', 32, 128, step=32)\n",
    "        dense_layer_size = trial.suggest_int('dense_layer_size', 16, 128, step=16)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "\n",
    "        # Define the model with suggested hyperparameters\n",
    "        model = regression_model_builder(\n",
    "            input_shape=renewables_df_train_scaled.shape,\n",
    "            first_lstm_layer_size=first_lstm_layer_size,\n",
    "            second_lstm_layer_size=second_lstm_layer_size,\n",
    "            dense_layer_size=dense_layer_size,\n",
    "            kernel_initializer=KERNEL_INITIALIZER,\n",
    "            recurrent_initializer=RECURRENT_INITIALIZER\n",
    "        )\n",
    "\n",
    "        # Compile the model\n",
    "        # log_cosh loss is used as it's robust for regression problems\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                      loss='log_cosh',   # Log-cosh loss: smooth approximation of absolute error, less sensitive to outliers\n",
    "                      metrics=['mse', 'mae'])\n",
    "\n",
    "        # Train the model with early stopping to prevent overfitting\n",
    "        model.fit(renewables_X_train, renewables_y_train, \n",
    "                  epochs=200,\n",
    "                  batch_size=64,\n",
    "                  validation_data=(renewables_X_val, renewables_y_val),\n",
    "                  callbacks=[generate_early_stop()],\n",
    "                  verbose=0)\n",
    "\n",
    "        # Evaluate the model on validation data and return the loss\n",
    "        return model.evaluate(renewables_X_val, renewables_y_val, verbose=0)[0]\n",
    "\n",
    "    # Create an Optuna study for hyperparameter optimization\n",
    "    study: optuna.study.Study = optuna.create_study(direction='minimize', study_name=\"renewables_model\")\n",
    "    study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "    # Train the final model with the best hyperparameters\n",
    "    renewables_best_params: Dict[str, Any] = study.best_params\n",
    "    best_model: keras.Model = regression_model_builder(\n",
    "        input_shape=renewables_df_train_scaled.shape,\n",
    "        first_lstm_layer_size=renewables_best_params[\"first_lstm_layer_size\"],\n",
    "        second_lstm_layer_size=renewables_best_params[\"second_lstm_layer_size\"],\n",
    "        dense_layer_size=renewables_best_params[\"dense_layer_size\"],\n",
    "        kernel_initializer=KERNEL_INITIALIZER,\n",
    "        recurrent_initializer=RECURRENT_INITIALIZER\n",
    "    )\n",
    "\n",
    "    # Compile with the best learning rate\n",
    "    best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=renewables_best_params['learning_rate']), \n",
    "                       loss='log_cosh', metrics=['mse', 'mae'])\n",
    "    \n",
    "    # Train the final model, saving the best weights\n",
    "    best_model.fit(renewables_X_train[:, :, 0], renewables_y_train,\n",
    "                   epochs=200, \n",
    "                   batch_size=64, \n",
    "                   validation_data=(renewables_X_val, renewables_y_val),\n",
    "                   callbacks=[RENEWABLES_CHECKPOINT, generate_early_stop()])\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_best: np.ndarray = best_model.predict(renewables_X_test)\n",
    "\n",
    "    y_pred_best_reshaped = y_pred_best.flatten().reshape(-1, 1)\n",
    "    y_test_reshaped = renewables_y_test.flatten().reshape(-1, 1)\n",
    "\n",
    "    y_pred_best_original = renewables_scaler.inverse_transform(y_pred_best_reshaped)\n",
    "    y_test_original = renewables_scaler.inverse_transform(renewables_y_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mape_best = np.mean(np.abs((y_test_original - y_pred_best_original) / y_test_original)) * 100\n",
    "    r2_best = r2_score(y_test_original, y_pred_best_original)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f\"Best model results:\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape_best:.3f}%\")\n",
    "    print(f\"R-squared (R2): {r2_best}\")\n",
    "\n",
    "    # Save the best hyperparameters to a JSON file for later use\n",
    "    hyperparameters_filename = f\"{PATH}/best_hyperparameters_renewable.json\"\n",
    "    with open(hyperparameters_filename, 'w') as json_file:\n",
    "        json.dump(renewables_best_params, json_file, indent=4)\n",
    "\n",
    "    print(f\"Best hyperparameters for 'renewable energy production' saved to {hyperparameters_filename}\")\n",
    "\n",
    "else:\n",
    "    # If not fine-tuning, load the previously saved best hyperparameters\n",
    "    hyperparameters_filename = f\"{PATH}/best_hyperparameters_renewable.json\"\n",
    "    with open(hyperparameters_filename, 'r') as json_file:\n",
    "        renewables_best_params: Dict[str, Any] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-renewables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TO_FINE_TUNE:\n",
    "    print(f\"Fine tunning 'non-renewable energy production' time series\")\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        first_lstm_layer_size = trial.suggest_int('first_lstm_layer_size', 32, 256, step=32)\n",
    "        second_lstm_layer_size = trial.suggest_int('second_lstm_layer_size', 32, 128, step=32)\n",
    "        dense_layer_size = trial.suggest_int('dense_layer_size', 16, 128, step=16)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "\n",
    "        # Create model with the suggested hyperparameters\n",
    "        model = regression_model_builder(\n",
    "            input_shape=non_renewables_df_train_scaled.shape,\n",
    "            first_lstm_layer_size=first_lstm_layer_size,\n",
    "            second_lstm_layer_size=second_lstm_layer_size,\n",
    "            dense_layer_size=dense_layer_size,\n",
    "            kernel_initializer=KERNEL_INITIALIZER,\n",
    "            recurrent_initializer=RECURRENT_INITIALIZER\n",
    "        )\n",
    "\n",
    "        # Compile model with Adam optimizer and log-cosh loss function\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='log_cosh', metrics=['mse', 'mae'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(non_renewables_X_train, non_renewables_y_train, epochs=200, batch_size=64, validation_data=(non_renewables_X_val, non_renewables_y_val), callbacks=[generate_early_stop()], verbose=0)\n",
    "\n",
    "        # Return validation loss\n",
    "        return model.evaluate(non_renewables_X_val, non_renewables_y_val, verbose=0)[0]\n",
    "\n",
    "    # Create and run the Optuna study with 200 trials\n",
    "    study: optuna.study.Study = optuna.create_study(direction='minimize', study_name=\"non_renewables_model\")\n",
    "    study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "    # Train a final model with the best hyperparameters\n",
    "    non_renewables_best_params: Dict[str, Any] = study.best_params\n",
    "    best_model: keras.Model = regression_model_builder(\n",
    "        input_shape=non_renewables_df_train_scaled.shape,\n",
    "        first_lstm_layer_size=non_renewables_best_params[\"first_lstm_layer_size\"],\n",
    "        second_lstm_layer_size=non_renewables_best_params[\"second_lstm_layer_size\"],\n",
    "        dense_layer_size=non_renewables_best_params[\"dense_layer_size\"],\n",
    "        kernel_initializer=KERNEL_INITIALIZER,\n",
    "        recurrent_initializer=RECURRENT_INITIALIZER\n",
    "    )\n",
    "\n",
    "    # Compile and train the best model\n",
    "    # Uses checkpointing to save the best model weights during training\n",
    "    best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=non_renewables_best_params['learning_rate']), loss='log_cosh', metrics=['mse', 'mae'])\n",
    "    best_model.fit(non_renewables_X_train[:, :, 0], non_renewables_y_train, epochs=200, batch_size=64, validation_data=(non_renewables_X_val, non_renewables_y_val), callbacks=[NON_RENEWABLES_CHECKPOINT, generate_early_stop()])\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred_best: np.ndarray = best_model.predict(non_renewables_X_test)\n",
    "\n",
    "    # Reshape predictions and true values for inverse scaling\n",
    "    y_pred_best_reshaped = y_pred_best.flatten().reshape(-1, 1)\n",
    "    y_test_reshaped = non_renewables_y_test.flatten().reshape(-1, 1)\n",
    "\n",
    "    # Convert scaled predictions back to original scale for meaningful evaluation\n",
    "    y_pred_best_original = non_renewables_scaler.inverse_transform(y_pred_best_reshaped)\n",
    "    y_test_original = non_renewables_scaler.inverse_transform(non_renewables_y_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mape_best = np.mean(np.abs((y_test_original - y_pred_best_original) / y_test_original)) * 100\n",
    "    r2_best = r2_score(y_test_original, y_pred_best_original)\n",
    "\n",
    "    # Display final performance metrics\n",
    "    print(f\"Best model results:\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape_best:.3f}%\")\n",
    "    print(f\"R-squared (R2): {r2_best}\")\n",
    "\n",
    "    # Save the best hyperparameters to JSON for future use\n",
    "    # This enables model reproducibility without rerunning optimization\n",
    "    hyperparameters_filename = f\"{PATH}/best_hyperparameters_non_renewable.json\"\n",
    "    with open(hyperparameters_filename, 'w') as json_file:\n",
    "        json.dump(non_renewables_best_params, json_file, indent=4)\n",
    "\n",
    "    print(f\"Best hyperparameters for 'non-renewable energy production' saved to {hyperparameters_filename}\")\n",
    "\n",
    "    # Clean up the Optuna study to free resources\n",
    "    optuna.delete_study(study_name=\"non_renewables_model\")\n",
    "\n",
    "else:\n",
    "    # If not fine-tuning, load previously saved best hyperparameters\n",
    "    hyperparameters_filename = f\"{PATH}/best_hyperparameters_non_renewable.json\"\n",
    "    with open(hyperparameters_filename, 'r') as json_file:\n",
    "        non_renewables_best_params: Dict[str, Any] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmarking and Performance Analysis\n",
    "\n",
    "### Benchmark Model Fit\n",
    "\n",
    "After finding the optimal hyperparameters, we need to assess the stability and reliability of our models. Deep learning models can show performance variability due to random initialization, training data batching, and optimization dynamics.\n",
    "\n",
    "This section implements a comprehensive benchmarking approach by:\n",
    "1. Training both renewable and non-renewable energy models multiple times (60 iterations)\n",
    "2. Measuring performance metrics (MAPE, R²) and training duration for each run\n",
    "3. Storing results for statistical analysis\n",
    "\n",
    "This benchmarking allows us to establish confidence intervals for expected model performance rather than relying on a single training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TO_BENCHMARK:\n",
    "    # Define columns for storing benchmark results\n",
    "    columns = ['attempt', 'mape', 'r2', 'training duration']\n",
    "    renewables_model_performance = []\n",
    "    non_renewables_model_performance = []\n",
    "\n",
    "    # Number of training iterations for reliable statistics\n",
    "    n_iterations = 60\n",
    "\n",
    "    # Benchmark the renewables energy forecasting model\n",
    "    print(\"Renewables time series\")\n",
    "    for i in range(n_iterations):\n",
    "        print(f\"Attempt {i + 1}..\")\n",
    "\n",
    "        # Create model with best hyperparameters found during optimization\n",
    "        model: keras.Model = regression_model_builder(\n",
    "            input_shape=renewables_df_train_scaled.shape,\n",
    "            first_lstm_layer_size=renewables_best_params[\"first_lstm_layer_size\"],\n",
    "            second_lstm_layer_size=renewables_best_params[\"second_lstm_layer_size\"],\n",
    "            dense_layer_size=renewables_best_params[\"dense_layer_size\"]\n",
    "        )\n",
    "\n",
    "        # Measure training time for performance benchmarking\n",
    "        start_ts: float = time.time()\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=renewables_best_params['learning_rate']), loss='log_cosh', metrics=['mse', 'mae'])\n",
    "        model.fit(renewables_X_train[:, :, 0], renewables_y_train, epochs=200, batch_size=64, validation_data=(renewables_X_val, renewables_y_val), callbacks=[RENEWABLES_CHECKPOINT, generate_early_stop()], verbose=0)\n",
    "        training_duration: float = time.time() - start_ts   # duration in seconds\n",
    "\n",
    "        # Generate predictions and evaluate model performance\n",
    "        y_pred_best: np.ndarray = model.predict(renewables_X_test)\n",
    "\n",
    "        # Reshape for inverse transformation\n",
    "        y_pred_reshaped = y_pred_best.flatten().reshape(-1, 1)\n",
    "        y_test_reshaped = renewables_y_test.flatten().reshape(-1, 1)\n",
    "\n",
    "        # Convert back to original scale\n",
    "        y_pred_original = renewables_scaler.inverse_transform(y_pred_reshaped)\n",
    "        y_test_original = renewables_scaler.inverse_transform(renewables_y_test)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "        r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "        # Store results for this iteration\n",
    "        renewables_model_performance.append([i + 1, mape, r2, training_duration])\n",
    "\n",
    "    # Benchmark the non-renewables energy forecasting model\n",
    "    print(\"Non-renewables time series\")\n",
    "    for i in range(n_iterations):\n",
    "        print(f\"Attempt {i + 1}..\")\n",
    "        \n",
    "        # Create model with best hyperparameters\n",
    "        model: keras.Model = regression_model_builder(\n",
    "            input_shape=non_renewables_df_train_scaled.shape,\n",
    "            first_lstm_layer_size=non_renewables_best_params[\"first_lstm_layer_size\"],\n",
    "            second_lstm_layer_size=non_renewables_best_params[\"second_lstm_layer_size\"],\n",
    "            dense_layer_size=non_renewables_best_params[\"dense_layer_size\"]\n",
    "        )\n",
    "\n",
    "        # Measure training time\n",
    "        start_ts: float = time.time()\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=non_renewables_best_params['learning_rate']), loss='log_cosh', metrics=['mse', 'mae'])\n",
    "        model.fit(non_renewables_X_train[:, :, 0], non_renewables_y_train, epochs=200, batch_size=64, validation_data=(non_renewables_X_val, non_renewables_y_val), callbacks=[RENEWABLES_CHECKPOINT, generate_early_stop()], verbose=0)\n",
    "        training_duration: float = time.time() - start_ts\n",
    "        \n",
    "        # Generate predictions and evaluate model performance\n",
    "        y_pred_best: np.ndarray = model.predict(non_renewables_X_test)\n",
    "\n",
    "        # Reshape and convert back to original scale\n",
    "        y_pred_reshaped = y_pred_best.flatten().reshape(-1, 1)\n",
    "        y_test_reshaped = non_renewables_y_test.flatten().reshape(-1, 1)\n",
    "        y_pred_original = non_renewables_scaler.inverse_transform(y_pred_reshaped)\n",
    "        y_test_original = non_renewables_scaler.inverse_transform(non_renewables_y_test)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "        r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "        # Store results for this iteration\n",
    "        non_renewables_model_performance.append([i + 1, mape, r2, training_duration])\n",
    "\n",
    "    # Convert results to DataFrames and save to CSV for later analysis\n",
    "    renewables_model_performance_df = pd.DataFrame(renewables_model_performance, columns=columns)\n",
    "    non_renewables_model_performance_df = pd.DataFrame(non_renewables_model_performance, columns=columns)\n",
    "\n",
    "    renewables_model_performance_df.to_csv(f\"{PATH}/renewables_model_performance.csv\", index=False)\n",
    "    non_renewables_model_performance_df.to_csv(f\"{PATH}/non_renewables_model_performance.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    # If not benchmarking, load previous benchmark results\n",
    "    renewables_model_performance_df = pd.read_csv(f\"{PATH}/renewables_model_performance.csv\")\n",
    "    non_renewables_model_performance_df = pd.read_csv(f\"{PATH}/non_renewables_model_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal for Robust Performance Estimation\n",
    "\n",
    "Deep learning models performance can sometimes be affected by good or bad initializations. To get a more robust estimate of expected performance, we apply statistical trimming to our benchmark results.\n",
    "\n",
    "In this section, we:\n",
    "1. Sort all benchmark results by MAPE (our primary performance metric)\n",
    "2. Remove the top and bottom 5% of results (potential outliers)\n",
    "3. Focus on the middle 90% to establish realistic performance expectations\n",
    "\n",
    "This approach provides a more reliable estimate of the model's performance in production settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by MAPE (lower is better) to identify outliers\n",
    "renewables_model_performance_df.sort_values(by='mape', inplace=True)\n",
    "non_renewables_model_performance_df.sort_values(by='mape', inplace=True)\n",
    "\n",
    "# Exclude the best 5% and worst 5% of results to remove potential outliers\n",
    "n = len(renewables_model_performance_df)\n",
    "lower_bound = int(n * 0.05)\n",
    "upper_bound = int(n * 0.95)\n",
    "renewables_model_performance_df = renewables_model_performance_df.iloc[lower_bound:upper_bound]\n",
    "non_renewables_model_performance_df = non_renewables_model_performance_df.iloc[lower_bound:upper_bound]\n",
    "\n",
    "# Reset indices after filtering\n",
    "renewables_model_performance_df.reset_index(drop=True, inplace=True)\n",
    "non_renewables_model_performance_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Summary\n",
    "\n",
    "After running multiple benchmark iterations, we can summarize the expected performance range for both our renewable and non-renewable energy forecasting models.\n",
    "\n",
    "The key metrics we analyze are:\n",
    "1. **MAPE (Mean Absolute Percentage Error)** - Lower values indicate better forecasting accuracy\n",
    "2. **R² (Coefficient of Determination)** - Higher values (closer to 100%) indicate the model explains more variance in the data\n",
    "3. **Training Duration** - Indicates computational requirements and training efficiency\n",
    "\n",
    "By providing ranges rather than single values, we offer a more realistic picture of expected model performance in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate range of performance metrics for renewable energy model\n",
    "renewables_mape_range = f\"[{round(renewables_model_performance_df[\"mape\"].min(), 3)}%, {round(renewables_model_performance_df[\"mape\"].max(), 3)}%]\"\n",
    "renewables_r2_range = f\"[{round(renewables_model_performance_df[\"r2\"].min()  * 100, 3)}%, {round(renewables_model_performance_df[\"r2\"].max()  * 100, 3)}%]\"\n",
    "renewables_duration_range = f\"[{round(renewables_model_performance_df[\"training duration\"].min(), 3)}s, {round(renewables_model_performance_df[\"training duration\"].max(), 3)}s]\"\n",
    "\n",
    "# Calculate range of performance metrics for non-renewable energy model\n",
    "non_renewables_mape_range = f\"[{round(non_renewables_model_performance_df[\"mape\"].min(), 3)}%, {round(non_renewables_model_performance_df[\"mape\"].max(), 3)}%]\"\n",
    "non_renewables_r2_range = f\"[{round(non_renewables_model_performance_df[\"r2\"].min()  * 100, 3)}%, {round(non_renewables_model_performance_df[\"r2\"].max()  * 100, 3)}%]\"\n",
    "non_renewables_duration_range = f\"[{round(non_renewables_model_performance_df[\"training duration\"].min(), 3)}s, {round(non_renewables_model_performance_df[\"training duration\"].max(), 3)}s]\"\n",
    "\n",
    "# Display performance ranges for renewable energy models\n",
    "print(\"Renewables model performance (with 90% of confidence):\")\n",
    "print(renewables_mape_range)\n",
    "print(renewables_r2_range)\n",
    "print(renewables_duration_range, \"\\n\")\n",
    "\n",
    "# Display performance ranges for non-renewable energy models\n",
    "print(\"Non-renewables model performance (with 90% of confidence):\")\n",
    "print(non_renewables_mape_range)\n",
    "print(non_renewables_r2_range)\n",
    "print(non_renewables_duration_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
